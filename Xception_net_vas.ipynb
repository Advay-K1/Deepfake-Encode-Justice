{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e538ad86-df7c-440c-b35f-8aed98117297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224d5262-7b8f-4295-b050-48513ecc780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vas_data_dir = os.path.join(\"Video2ImageDataset_noFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42ff4dc-260e-4c88-9239-9c7c4c460ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26460"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"Video2ImageDataset_noFace/fake\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed34c040-a437-4df0-9645-39815879af5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2812"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"Video2ImageDataset_noFace/real\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a541516a-e6b4-46ba-b846-f5063ee95d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split-folders\n",
      "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b45a2eea-dd87-488d-aac6-3a82389a0657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 29272 files [00:14, 1964.35 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio('Video2ImageDataset_noFace', output=\"output\", seed=1337, ratio=(.8, 0.1,0.1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab4200f4-312a-4687-a713-52e70f5eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2249, 21168)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"output/train/real\")), len(os.listdir(\"output/train/fake\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91550ff2-ae75-4b00-bdad-a769f1d77857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 2647)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"output/test/real\")), len(os.listdir(\"output/test/fake\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58c96ec1-8a9d-403c-a807-a58d670a96fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 2646)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"output/val/real\")), len(os.listdir(\"output/val/fake\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efdba9c5-2224-4157-b966-c5d1051e6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vas_train_data_path = \"output/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e21e1c10-a73d-438f-be7d-82c4d6ce4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23417 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vas_train_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "target_size = (224, 224) \n",
    "batch_size = 100\n",
    "\n",
    "vas_train_data_generator = vas_train_data_gen.flow_from_directory(\n",
    "    vas_train_data_path,\n",
    "    target_size= target_size, \n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary', \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2ef1909-7d5d-4a35-b929-53ddf2077aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 0, 'real': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vas_train_data_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e68cff84-4bfd-4192-9e57-676db5cff904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfake\u001b[m\u001b[m/ \u001b[34mreal\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls output/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fe37501-a7f1-48f6-8cd2-c5e9ee0394a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf output/train/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cfa1d15-d419-4d51-bc4f-836978177848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.legacy import Adam  \n",
    "\n",
    "def build_model():\n",
    "    shape = (224, 224, 3)\n",
    "    \n",
    "    Xception_mod_trained = Xception(input_shape=shape, include_top=False, weights='imagenet')  \n",
    "    \n",
    "    \n",
    "    final_model_transfer = Sequential()\n",
    "    \n",
    "    final_model_transfer.add(Xception_mod_trained)\n",
    "    \n",
    "    final_model_transfer.add(layers.GlobalAveragePooling2D())\n",
    "    final_model_transfer.add(layers.Dense(1024, activation='relu'))\n",
    "    final_model_transfer.add(layers.Dropout(0.5))\n",
    "    \n",
    "    final_model_transfer.add(layers.Dense(1, activation='sigmoid'))  \n",
    "    \n",
    "    final_model_transfer.compile(optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'], loss = 'binary_crossentropy')\n",
    "\n",
    "\n",
    "    return final_model_transfer\n",
    "\n",
    "\n",
    "model_trans = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2d105bf-c153-460f-987d-e73dee8119e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 2825s 12s/step - loss: 0.3464 - accuracy: 0.8984\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 2910s 12s/step - loss: 0.2973 - accuracy: 0.9040\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 3506s 15s/step - loss: 0.2731 - accuracy: 0.9040\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 2833s 12s/step - loss: 0.2441 - accuracy: 0.9049\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 2857s 12s/step - loss: 0.2115 - accuracy: 0.9090\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 2831s 12s/step - loss: 0.1781 - accuracy: 0.9236\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 2802s 12s/step - loss: 0.1446 - accuracy: 0.9392\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 2790s 12s/step - loss: 0.1112 - accuracy: 0.9545\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 2808s 12s/step - loss: 0.0834 - accuracy: 0.9668\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 2784s 12s/step - loss: 0.0650 - accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model_trans.fit(vas_train_data_generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d33ba95-3053-4b5c-8298-7d6825300aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2929 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vas_test_data_path = \"output/test\"\n",
    "\n",
    "vas_test_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "target_size = (224, 224) \n",
    "batch_size = 100\n",
    "\n",
    "vas_test_data_generator = vas_test_data_gen.flow_from_directory(\n",
    "    vas_test_data_path,\n",
    "    target_size= target_size, \n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary', \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e0f8a04-777c-4941-b42d-ea149ec46a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 0, 'real': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vas_test_data_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16b3547f-4117-4f6e-aebb-c58a46afe185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfake\u001b[m\u001b[m/ \u001b[34mreal\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls output/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63fae247-a066-4048-8389-7acc3acc9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf output/test/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80cad175-af93-4732-abbf-b0cee823f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 78s 3s/step - loss: 0.2754 - accuracy: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27537328004837036, 0.9248890280723572]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trans.evaluate(vas_test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48ab3c50-cec5-41eb-97cd-bd9716695c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trans.save(\"vas_transfer_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448a93f-9110-4343-b62f-c9906d1f5ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning-env] *",
   "language": "python",
   "name": "conda-env-machine-learning-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
